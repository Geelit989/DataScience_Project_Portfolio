# -*- coding: utf-8 -*-
"""ANA675_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_mNboE4TzSPkwfekR4eeph3INH0AAhdA
"""

####### Final Project, ML & DL #########

#Build 2 models

#Train 2 models

#Evaluate 2 models

#Compare and discuss 2 techniques used

# Read in data

import numpy as np
import pandas as pd

oil_spill = pd.read_csv('/content/oil_spill.csv',header=0)
oil_spill.iloc[:,22]

# Descriptive Stats

oil_spill.describe()

#oil_scaled.shape

oil_spill.info()

oil_spill.hist()

corr_matrix = oil_spill.corr()
print(corr_matrix)

# Visualize correlation matrix

import matplotlib.pyplot as plt
import seaborn as sn

sn.heatmap(corr_matrix, annot=True)
plt.show()

# Run PCA to reduce dimensionality but keep explained variance

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, MinMaxScaler

pca = PCA(n_components=25)

X = oil_spill.drop(columns=['target','f_23']) #Remove target and f_23 feature (all NANs)
Y = oil_spill['target']
scaler = StandardScaler()
scaler.fit(X)

oil_scaled = scaler.transform(X)

X_pca = pca.fit_transform(oil_scaled)

X_pca.shape # Shape of reduced data set, d=48 => d=25

X_pca[500] #Sample [500]

### SVC for Classification

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split


svc = SVC(C=1,random_state=70,probability=True) #Lowering C decreases model accuracy negatively, default = 1
svc.fit(X_pca, Y)
X['predictions'] = svc.predict(X_pca)

print('~~~~~~~~~~~~~~~~~~~~~~~~~~')
#Evaluate Classifier on ALL PCA data
score = svc.score(X_pca,Y)
print('PCA model accuracy score = ', score)


print('~~~~~~~~~~~~~~~~~~~~~~~~~~')
#Predict label
print('labeled prediction of X = ', Y[500])
print('predicted label of X =', X['predictions'][500])


#Build SVC non-PCA, test train split
print('~~~~~~~~~~~~~~~~~~~~~~~~~~')
x_train, x_test, y_train, y_test = train_test_split(oil_scaled,Y, test_size=.30, train_size=.70, random_state=10)
svc1 = SVC(random_state=70,probability=True) #Sigmoid lowers accuracy negatively, linear does not generalize well, poly lowers accuracy and increases generalizability, gamma=.08 almost 99% training accuracy


svc1.fit(x_train, y_train)
predictions_non_pca = svc1.predict(x_train)
score1 = svc1.score(x_train,y_train)
print('Non-PCA model training accuracy = ', score1)



print('~~~~~~~~~~~~~~~~~~~~~~~~~~')
#Evaluate non-PCA SVM on test data for generalizability
score2 = svc1.score(x_test,y_test)
test_predictions_non_pca = svc1.predict(x_test)
print('Non-PCA model test accuracy score = ', score2)


#Evaluate non-PCA test accuracy




print('~~~~~~~~~~~~~~~~~~~~~~~~~~')
#Predict non-PCA label
print('labeled prediction of X = ', Y[700])
print('predicted label of X =', X['predictions'][700])

# Confusion Matrix of SVC using PCA and No PCA

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


cm = confusion_matrix(y_true= y_train, y_pred= predictions_non_pca, labels=[0,1])
cm1 = confusion_matrix(y_true= y_test, y_pred= test_predictions_non_pca, labels=[0,1])


disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['0','1'])
disp1 = ConfusionMatrixDisplay(confusion_matrix=cm1, display_labels=['0','1'])

disp.plot()
disp1.plot()
plt.show()





##### Build ANN Classifier #########

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Normalization

#Define Input

inputs = keras.Input(shape=(48,))


#Add normalization Layer

normalizer = Normalization(axis=-1)
normalizer.adapt(x_train)
normalized_oil_train = normalizer(x_train)

normalizer.adapt(x_test)
normalized_oil_test = normalizer(x_test)

#Create layers

dense = layers.Dense(32, activation="relu")
x = dense(inputs)
x = layers.Dense(64, activation="relu")(x)

#Define output layer

num_classes = 2
outputs = layers.Dense(num_classes, activation="sigmoid")(x)

#Define model

model = keras.Model(inputs=inputs, outputs=outputs, name="Oil Spill ANN Model")

###### Build ANN 

model1 = keras.Sequential()
model1.add(keras.Input(shape=(48,))) # Vector is 48,
model1.add(layers.Dense(32, activation="relu"))#Started with 1 layer
model1.add(layers.Dense(64, activation="relu"))#Added 2nd layer with increased learning rate, increase training accuracy to 98.28%
model1.add(layers.Dense(1, activation="sigmoid",))

# Compile model

model1.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=.002), #defualt learning rate is .001, 98.09 training accuracy
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=["accuracy","mean_squared_error"],
)

#Fit Model on data

ann = model1.fit(
    normalized_oil_train,
    y_train,
    batch_size=64,
    validation_split=.2,
    epochs=11 #Started with 10 epochs = 95.9 test accuracy, increased to 20 epochs, 1 layer accuracy = 98%. Decreased to 11 epochs/1layer = 95.8%, 96% test. 
    )

#View loss and metrics per epoch

ann.history

# Evaluate Model on test data

print("Test data accuracy = ", model1.evaluate(normalized_oil_test,y_test, batch_size=50)), #1 layer test accuracy=95.3%

model1.summary()

keras.utils.plot_model(model1, show_shapes=True, rankdir="LR")

pd.DataFrame(ann.history).plot(figsize=(8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()









